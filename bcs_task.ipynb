{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import re\n",
        "import string\n",
        "\n",
        "# List of stop words\n",
        "stopwords = set(\"\"\"\n",
        "i me my myself we our ours ourselves you your yours yourself yourselves he him his himself she her hers herself it\n",
        "its itself they them their theirs themselves what which who whom this that these those am is are was were be been\n",
        "being have has had having do does did doing a an the and but if or because as until while of at by for with about\n",
        "against between into through during before after above below to from up down in out on off over under again further\n",
        "then once here there when where why how all any both each few more most other some such no nor not only own same so\n",
        "than too very s t can will just don should now\n",
        "\"\"\".split())\n",
        "\n",
        "\n",
        "def custom_tokenizer(text):\n",
        "    text = str(text).lower()\n",
        "\n",
        "    # Normalize repeated characters\n",
        "    def normalize_repeat(match):\n",
        "        char = match.group(1)\n",
        "        repeat_len = len(match.group(0))\n",
        "        return f\"{char} <REPEAT:{repeat_len}>\"\n",
        "\n",
        "    text = re.sub(r\"(.)\\1{2,}\", normalize_repeat, text)\n",
        "\n",
        "    # Remove punctuation\n",
        "    text = text.translate(str.maketrans('', '', string.punctuation))\n",
        "\n",
        "    # Split into words\n",
        "    tokens = text.split()\n",
        "\n",
        "    # Remove stopwords\n",
        "    tokens = [word for word in tokens if word not in stopwords]\n",
        "\n",
        "    return tokens\n",
        "\n",
        "# --- POS Tagger ---\n",
        "def enhanced_pos_tagger(tokens):\n",
        "    pos_tags = []\n",
        "    prepositions = {'in', 'on', 'at', 'by', 'to', 'with', 'from', 'about', 'into', 'over', 'under'}\n",
        "    conjunctions = {'and', 'but', 'or', 'so', 'because', 'although', 'if', 'while'}\n",
        "\n",
        "    for word in tokens:\n",
        "        if word in prepositions:\n",
        "            tag = \"PREPOSITION\"\n",
        "        elif word in conjunctions:\n",
        "            tag = \"CONJUNCTION\"\n",
        "        elif word.endswith(\"ing\") or word.endswith(\"ed\"):\n",
        "            tag = \"VERB\"\n",
        "        elif word.endswith(\"ly\"):\n",
        "            tag = \"ADVERB\"\n",
        "        elif word.endswith(\"ful\") or word.endswith(\"ous\") or word.endswith(\"able\"):\n",
        "            tag = \"ADJECTIVE\"\n",
        "        elif word.endswith(\"tion\") or word.endswith(\"ness\") or word.endswith(\"ment\"):\n",
        "            tag = \"NOUN\"\n",
        "        else:\n",
        "            tag = \"OTHER\"\n",
        "        pos_tags.append((word, tag))\n",
        "    return pos_tags\n",
        "\n",
        "# --- Lemmatizer ---\n",
        "def lemmatizer(pos_tagged_tokens):\n",
        "    lemmas = []\n",
        "    custom_lemmas = {\n",
        "        \"beautiful\": \"beauty\", \"hopeful\": \"hope\", \"joyful\": \"joy\", \"useful\": \"use\", \"careful\": \"care\"\n",
        "    }\n",
        "\n",
        "    for word, tag in pos_tagged_tokens:\n",
        "        if tag in [\"PREPOSITION\", \"CONJUNCTION\"]:\n",
        "            continue\n",
        "\n",
        "        if word in custom_lemmas:\n",
        "            lemma = custom_lemmas[word]\n",
        "        else:\n",
        "            lemma = word\n",
        "            if tag == \"VERB\":\n",
        "                if word.endswith(\"ing\"):\n",
        "                    lemma = word[:-3]\n",
        "                elif word.endswith(\"ed\"):\n",
        "                    lemma = word[:-2]\n",
        "            elif tag == \"ADJECTIVE\":\n",
        "                if word.endswith(\"ful\"):\n",
        "                    lemma = word[:-3] + \"y\"\n",
        "                elif word.endswith(\"ous\"):\n",
        "                    lemma = word[:-3]\n",
        "                elif word.endswith(\"able\"):\n",
        "                    lemma = word[:-4]\n",
        "            elif tag == \"ADVERB\":\n",
        "                if word.endswith(\"ly\"):\n",
        "                    lemma = word[:-2] + \"y\"\n",
        "\n",
        "        lemmas.append(lemma)\n",
        "    return lemmas\n"
      ],
      "metadata": {
        "id": "dAkaQcIzfQqN"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load your CSV file\n",
        "df = pd.read_csv(\"/content/merged_news.csv\")  # Replace with your CSV filename\n",
        "\n",
        "# Process each row\n",
        "def process_text(text):\n",
        "    tokens = custom_tokenizer(text)\n",
        "    tagged = enhanced_pos_tagger(tokens)\n",
        "    lemmas = lemmatizer(tagged)\n",
        "    return \" \".join(lemmas)\n",
        "\n",
        "# Apply to the 'text' column\n",
        "df[\"lemmas\"] = df[\"text\"].apply(process_text)\n",
        "\n",
        "# Save result\n",
        "df.to_csv(\"/content/merged_news_lemma.csv\", index=False)\n",
        "print(\"âœ… Done! Saved as 'processed_output.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oHcpMYkwgTYO",
        "outputId": "34553fe6-a31f-4277-9745-66ffd774f479"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… Done! Saved as 'processed_output.csv'\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from collections import Counter\n",
        "from scipy.sparse import lil_matrix, diags\n",
        "\n",
        "# Step 1: Load CSV and tokenize lemmas\n",
        "df = pd.read_csv(\"/content/merged_news_lemma.csv\")\n",
        "df[\"lemmas\"] = df[\"lemmas\"].astype(str).apply(lambda x: x.split())\n",
        "\n",
        "# Step 2: Build vocabulary and mappings\n",
        "vocab = sorted(set(word for doc in df[\"lemmas\"] for word in doc))\n",
        "word_to_index = {word: i for i, word in enumerate(vocab)}\n",
        "index_to_word = {i: word for word, i in word_to_index.items()}\n",
        "\n",
        "# Step 3: Compute sparse TF matrix\n",
        "num_docs = len(df)\n",
        "vocab_size = len(vocab)\n",
        "tf_matrix = lil_matrix((num_docs, vocab_size))\n",
        "\n",
        "for i, doc in enumerate(df[\"lemmas\"]):\n",
        "    word_counts = Counter(doc)\n",
        "    total_words = len(doc)\n",
        "    for word, count in word_counts.items():\n",
        "        if word in word_to_index:\n",
        "            tf_matrix[i, word_to_index[word]] = count / total_words\n",
        "\n",
        "# ðŸ“Š Step 4: Compute IDF vector\n",
        "df_vector = np.zeros(vocab_size)\n",
        "for word, idx in word_to_index.items():\n",
        "    df_vector[idx] = sum(1 for doc in df[\"lemmas\"] if word in doc)\n",
        "\n",
        "idf_vector = np.log((num_docs + 1) / (df_vector + 1)) + 1  # smoothed IDF\n",
        "idf_diag = diags(idf_vector)\n",
        "\n",
        "# ðŸ§  Step 5: Compute TF-IDF matrix\n",
        "tfidf_matrix = tf_matrix @ idf_diag  # Sparse matrix multiplication\n",
        "\n",
        "# ðŸ“¤ Step 6: Convert to dense DataFrame (only if small enough)\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vocab)\n",
        "result_df = pd.concat([df[[\"title\", \"label\"]].reset_index(drop=True), tfidf_df], axis=1)\n",
        "\n",
        "# ðŸ’¾ Step 7: Save to CSV\n",
        "result_df.to_csv(r\"/content/tf-idf-merged_news.csv\", index=False)\n",
        "print(\"TF-IDF features saved in 'tf-idf_lemmas.csv'\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 332
        },
        "id": "ugqinZwnfh_s",
        "outputId": "4dae7b15-4f9b-4d2d-f2bc-940773e4b242"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-17-58354a01358a>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdf_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdf_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lemmas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0midf_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_docs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_vector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# smoothed IDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-17-58354a01358a>\u001b[0m in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     28\u001b[0m \u001b[0mdf_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzeros\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvocab_size\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mword\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mword_to_index\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mitems\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 30\u001b[0;31m     \u001b[0mdf_vector\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msum\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"lemmas\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mword\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mdoc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     31\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     32\u001b[0m \u001b[0midf_vector\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlog\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_docs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdf_vector\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m  \u001b[0;31m# smoothed IDF\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "\n",
        "# Step 1: Load CSV and prepare data\n",
        "df = pd.read_csv(\"/content/merged_news_lemma.csv\")\n",
        "\n",
        "# Ensure lemmas column is tokenized list; join for vectorizer\n",
        "df[\"lemmas\"] = df[\"lemmas\"].astype(str).apply(lambda x: x.split())\n",
        "df[\"joined_lemmas\"] = df[\"lemmas\"].apply(lambda x: \" \".join(x))\n",
        "\n",
        "# Step 2: TF-IDF Vectorization with max_features to limit memory usage\n",
        "vectorizer = TfidfVectorizer(\n",
        "    tokenizer=lambda x: x.split(),  # Already tokenized\n",
        "    lowercase=False,\n",
        "    norm='l2',\n",
        "    smooth_idf=True,\n",
        "    sublinear_tf=False,\n",
        "    max_features=10000  # Limit to top 10,000 words\n",
        ")\n",
        "\n",
        "tfidf_matrix = vectorizer.fit_transform(df[\"joined_lemmas\"])\n",
        "\n",
        "# Step 3: Convert to DataFrame\n",
        "tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), columns=vectorizer.get_feature_names_out())\n",
        "\n",
        "# Step 4: Combine with title and label\n",
        "result_df = pd.concat([df[[\"title\", \"label\"]].reset_index(drop=True), tfidf_df], axis=1)\n",
        "\n",
        "# Step 5: Save to CSV\n",
        "result_df.to_csv(\"/content/tf-idf-merged_news.csv\", index=False)\n",
        "print(\" TF-IDF features saved using Scikit-learn (limited to 10,000 words)\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GKStvC-i5MjJ",
        "outputId": "a9a6ea0b-ac4f-4461-9cc0-710b65b99beb"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/sklearn/feature_extraction/text.py:517: UserWarning: The parameter 'token_pattern' will not be used since 'tokenizer' is not None'\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "âœ… TF-IDF features saved using Scikit-learn (limited to 10,000 words)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import classification_report, confusion_matrix, f1_score, accuracy_score\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "\n",
        "# Step 1: Load data\n",
        "df = pd.read_csv(\"/content/tf-idf-merged_news.csv\")\n",
        "\n",
        "# Step 2: Features and target\n",
        "X = df.drop(columns=['title', 'label'])\n",
        "y = df['label']\n",
        "\n",
        "# Step 3: Train-test split\n",
        "X_train, X_test, y_train, y_test = train_test_split(\n",
        "    X, y, test_size=0.2, stratify=y, random_state=42\n",
        ")\n",
        "\n",
        "# Optional: scale features (TF-IDF is already normalized but this is safe)\n",
        "scaler = StandardScaler(with_mean=False)  # with_mean=False to avoid error on sparse TF-IDF\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Step 4: Train Logistic Regression\n",
        "lr = LogisticRegression(max_iter=1000, class_weight='balanced', random_state=42)\n",
        "lr.fit(X_train_scaled, y_train)\n",
        "\n",
        "# Step 5: Evaluate\n",
        "y_pred = lr.predict(X_test_scaled)\n",
        "\n",
        "print(\"ðŸ”¹ Confusion Matrix:\\n\", confusion_matrix(y_test, y_pred))\n",
        "print(\"\\nðŸ”¹ Classification Report:\\n\", classification_report(y_test, y_pred, digits=4))\n",
        "print(\"ðŸ”¹ F1 Score:\", f1_score(y_test, y_pred, average='weighted'))\n",
        "print(\"ðŸ”¹ Accuracy:\", accuracy_score(y_test, y_pred))\n"
      ],
      "metadata": {
        "id": "UlGvcOe-5iS5"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}